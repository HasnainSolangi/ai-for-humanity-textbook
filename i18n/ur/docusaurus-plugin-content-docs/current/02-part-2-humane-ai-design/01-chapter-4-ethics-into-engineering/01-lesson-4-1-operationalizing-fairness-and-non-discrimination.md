---
id: 01-lesson-4-1-operationalizing-fairness-and-non-discrimination
title: "انصاف پسندی اور عدم امتیاز کا عملی نفاذ"
sidebar_label: "انصاف پسندی اور عدم امتیاز کا عملی نفاذ"
---

# انصاف پسندی اور عدم امتیاز کا عملی نفاذ

انسانیت کی خدمت کرنے والی مصنوعی ذہانت (AI) کی تلاش میں، انصاف پسندی (Fairness) کوئی ثانوی خیال نہیں ہونا چاہیے—بلکہ یہ ایک بنیادی انجینئرنگ کی ضرورت ہونی چاہیے۔ چونکہ اے آئی سسٹمز روزگار، قرض، صحت کی دیکھ بھال، اور رہائش جیسے اہم مواقع تک رسائی کے لیے تیزی سے "دربان" (gatekeeper) کا کردار ادا کر رہے ہیں، اس لیے تاریخی ناانصافیوں کو خودکار بنانے اور انہیں بڑھانے کا خطرہ شدید ہوتا جا رہا ہے۔ یہ سبق ان عملی، تکنیکی اور اخلاقی فریم ورکس کا جائزہ لیتا ہے جو محض تجریدی اصولوں سے آگے بڑھ کر مشین لرننگ کے لائف سائیکل میں انصاف پسندی کو عملی شکل دینے کے لیے ضروری ہیں۔

## "لاعلمی کے ذریعے انصاف پسندی" سے آگے بڑھنا

ابتدائی اے آئی کی ترقی میں ایک عام غلط فہمی "لاعلمی کے ذریعے انصاف پسندی" (fairness through unawareness) کا تصور تھا—یعنی یہ عقیدہ کہ ڈیٹا سیٹ سے نسل، جنس، یا عمر جیسی حساس معلومات کو ہٹا دینے سے امتیازی نتائج کو روکا جا سکتا ہے۔ اب ہم سمجھتے ہیں کہ یہ تصور بنیادی طور پر ناقص ہے۔ اے آئی ماڈلز پیٹرن تلاش کرنے والے انتہائی طاقتور انجن ہیں؛ وہ حساس صفات کے "متبادلات" (proxies) کو آسانی سے پہچان سکتے ہیں۔ مثال کے طور پر، ایک ماڈل زپ کوڈ کو نسل کے متبادل کے طور پر، یا براؤزنگ ہسٹری/ذخیرہ الفاظ کو عمر یا جنس کے متبادل کے طور پر استعمال کر سکتا ہے۔ حقیقی معنوں میں منصفانہ نظام بنانے کے لیے، ہمیں تعصب کے نہ ہونے کا ڈھونگ رچانے کے بجائے واضح طور پر اس کی پیمائش اور اس میں تخفیف کرنی چاہیے۔

## انصاف پسندی کی انجینئرنگ کا لائف سائیکل

انصاف پسندی کو عملی شکل دینے کے لیے کوڈ لکھنے سے پہلے ہی ترقی کے ہر مرحلے میں ایک مسلسل اور تکراری عمل کی ضرورت ہوتی ہے:

1.  **ہدف کی تعریف اور مسئلے کی تشکیل:**
    تعصب اکثر سسٹم میں کوڈ کی پہلی لائن لکھے جانے سے پہلے ہی داخل ہو جاتا ہے۔ انجینئرز کو اس "ٹارگٹ ویری ایبل" (target variable) کی سختی سے جانچ کرنی چاہیے جس کے لیے وہ سسٹم بنا رہے ہیں۔ مثال کے طور پر، صحت کی دیکھ بھال میں، "مریض کی صحت کی ضروریات" کے متبادل کے طور پر "صحت کی دیکھ بھال کی لاگت" کا استعمال فطرتی طور پر متعصبانہ ہے، کیونکہ پسماندہ آبادی رسائی کی کمی کی وجہ سے اکثر کم اخراجات کرتی ہے، نہ کہ بہتر صحت کی وجہ سے۔ انصاف پسندی کو بہتر بنانے کا آغاز ان اہداف کی تعریف سے ہوتا ہے جو محض آسان ڈیٹا پوائنٹس کے بجائے حقیقی انسانی نتائج کی عکاسی کریں۔

2.  **ڈیٹا کی تیاری اور نمائندگی:**
    ڈیٹا معاشرے کا آئینہ ہے، اور معاشرہ نامکمل ہے۔ "آلودہ" ڈیٹا متعصبانہ ماڈلز کا باعث بنتا ہے۔
    *   **نمائندہ نمونہ سازی (Representative Sampling):** فعال طور پر اس بات کو یقینی بنانا کہ تربیتی ڈیٹا سیٹس اس حقیقی دنیا کی آبادی کے تنوع کی عکاسی کریں جس کی سسٹم خدمت کرے گا۔
    *   **تعصب کی نشاندہی:** انڈر ریپریزنٹیڈ (underrepresented) گروہوں یا لیبلز میں موجود تاریخی تعصبات کی شناخت کے لیے تربیتی ڈیٹا کا سخت شماریاتی تجزیہ۔
    *   **مصنوعی اضافہ (Synthetic Augmentation):** رازداری کے تحفظ کے حامل مصنوعی ڈیٹا (synthetic data) تیار کرنے کے لیے جدید تکنیکوں کا استعمال کرنا تاکہ اقلیتی گروہوں کی نمائندگی کو متوازن کیا جا سکے، اس بات کو یقینی بناتے ہوئے کہ ماڈل تمام صارفین کے لیے مضبوط نمونے سیکھے۔

3.  **الگورتھمک تخفیف (Algorithmic Mitigation):**
    تربیتی عمل کے دوران، مخصوص ریاضیاتی مداخلتیں ماڈل کو انصاف پسندی کی طرف موڑ سکتی ہیں۔
    *   **Pre-processing:** تربیت شروع ہونے سے پہلے عدم توازن کو دور کرنے کے لیے ڈیٹا کو دوبارہ وزن دینا یا دوبارہ نمونہ بنانا۔
    *   **In-processing:** ماڈل کے "لاس فنکشن" (loss function) میں ترمیم کرنا تاکہ امتیازی پیٹرنز پر جرمانہ عائد کیا جا سکے۔ یہ الگورتھم کو تعصب کے ذریعے حاصل کی گئی درستگی کے لیے "قیمت ادا کرنے" پر مجبور کرتا ہے۔
    *   **Post-processing:** ماڈل کی تربیت کے بعد فیصلے کی حدوں (decision thresholds) کو ایڈجسٹ کرنا تاکہ مختلف آبادیاتی گروہوں میں منصفانہ نتائج کی شرح کو یقینی بنایا جا سکے۔

## انصاف پسندی میں پیمائش اور سمجھوتے

"انصاف پسندی" کی کوئی ایک ریاضیاتی تعریف نہیں ہے۔ مختلف حالات مختلف پیمانوں کا تقاضا کرتے ہیں، اور ان سب کو بیک وقت پورا کرنا اکثر ریاضیاتی طور پر ناممکن ہوتا ہے۔ عملی اطلاق کے لیے واضح اور دستاویزی سمجھوتے (trade-offs) کرنے کی ضرورت ہوتی ہے:

*   **ڈیموگرافک پیریٹی (Demographic Parity):** اس بات کو یقینی بنانا کہ مثبت نتائج (مثلاً قرض حاصل کرنا) مختلف گروہوں میں برابر کی شرح سے دیے جائیں۔ اسے اکثر تاریخی سسٹمک رکاوٹوں کی تلافی کے لیے استعمال کیا جاتا ہے۔
*   **برابر کے امکانات (Equalized Odds):** اس بات کو یقینی بنانا کہ غلطیوں کی شرح (فلز پوزیٹو اور فالس نیگیٹو) تمام گروہوں میں یکساں ہو۔ یہ ان صورتوں کو روکتا ہے جہاں ایک گروہ کو غیر منصفانہ طور پر سزا دی جائے یا غیر منصفانہ طور پر باہر رکھا جائے۔
*   **کیلیبریشن (Calibration):** اس بات کو یقینی بنانا کہ ایک رسک اسکور کا مطلب سب کے لیے ایک ہی ہو۔ "70% خطرہ" کا مطلب ایونٹ وقوع پذیر ہونے کا وہی امکان ہونا چاہیے، چاہے فرد کا پس منظر کچھ بھی ہو۔

## مسلسل نگرانی اور ریڈ ٹیمنگ

انصاف پسندی محض ایک بار کا تعمیل کا کام نہیں ہے؛ یہ ایک زندہ نظام کی ایک متحرک خصوصیت ہے۔
*   **ڈرفٹ ڈیٹیکشن (Drift Detection):** آبادی کی ساخت بدلتی ہے اور صارفین کے رویے تبدیل ہوتے ہیں۔ ایک ماڈل جو آج منصفانہ ہے وہ کل متعصب ہو سکتا ہے۔ مانیٹرنگ ڈیش بورڈز کو کارکردگی کے ساتھ ساتھ انصاف پسندی کے پیمانوں پر بھی نظر رکھنی چاہیے۔
*   **ایڈورسریل ریڈ ٹیمنگ (Adversarial Red Teaming):** ماہرین اخلاقیات اور سماجی سائنسدانوں کی مخصوص ٹیمیں جو فعال طور پر ماڈل کو "توڑنے" کی کوشش کرتی ہیں—بشمول ایسے کیسز تلاش کرنا جہاں یہ تعصب کا مظاہرہ کرتا ہے یا نقصان دہ سٹیریو ٹائپس پیدا کرتا ہے—اس سے پہلے کہ سسٹم عوامی سطح پر نافذ کیا جائے۔

انصاف پسندی کو سسٹم کی ایک بنیادی خوبی (جیسے اسکیل ایبلٹی، سیکورٹی، یا کارکردگی) کے طور پر دیکھ کر، ادارے ایسی اے آئی بنا سکتے ہیں جو اعتماد حاصل کرے اور ماضی کے تعصبات کو خودکار بنانے کے بجائے انصاف اور برابری کو فروغ دے۔